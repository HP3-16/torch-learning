{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import opendatasets as od\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchtext.data import Field,LabelField,TabularDataset,BucketIterator\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.vocab import GloVe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1.11.0+cu113\n",
      "0.6.0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "print(device)\n",
    "print(torch.__version__)\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#od.download('https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = pd.read_csv('D:/py-py-py-pytorch/rnn/fake-and-real-news-dataset/True.csv')\n",
    "fake_df = pd.read_csv('D:/py-py-py-pytorch/rnn/fake-and-real-news-dataset/Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df.drop(['date','subject'],axis=1,inplace=True)\n",
    "fake_df.drop(['date','subject'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df['Label'] = 1.0\n",
    "fake_df['Label'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df['TEXT'] = real_df['title']+real_df['text']\n",
    "real_df.drop(['title','text'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df['TEXT'] = fake_df['title']+fake_df['text']\n",
    "fake_df.drop(['title','text'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               TEXT\n",
       "0    1.0  As U.S. budget fight looms, Republicans flip t...\n",
       "1    1.0  U.S. military to accept transgender recruits o...\n",
       "2    1.0  Senior U.S. Republican senator: 'Let Mr. Muell..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               TEXT\n",
       "0    0.0   Donald Trump Sends Out Embarrassing New Year’...\n",
       "1    0.0   Drunk Bragging Trump Staffer Started Russian ...\n",
       "2    0.0   Sheriff David Clarke Becomes An Internet Joke..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>1.0</td>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>1.0</td>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Minsk cultural hub becomes haven from authorit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label                                               TEXT\n",
       "0        0.0   Donald Trump Sends Out Embarrassing New Year’...\n",
       "1        0.0   Drunk Bragging Trump Staffer Started Russian ...\n",
       "2        0.0   Sheriff David Clarke Becomes An Internet Joke...\n",
       "3        0.0   Trump Is So Obsessed He Even Has Obama’s Name...\n",
       "4        0.0   Pope Francis Just Called Out Donald Trump Dur...\n",
       "...      ...                                                ...\n",
       "44893    1.0  'Fully committed' NATO backs new U.S. approach...\n",
       "44894    1.0  LexisNexis withdrew two products from Chinese ...\n",
       "44895    1.0  Minsk cultural hub becomes haven from authorit...\n",
       "44896    1.0  Vatican upbeat on possibility of Pope Francis ...\n",
       "44897    1.0  Indonesia to buy $1.14 billion worth of Russia...\n",
       "\n",
       "[44898 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.concat([fake_df,real_df],ignore_index=True)\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.rename(columns={'Label':'LABEL'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_df.to_csv('news_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['TEXT'] = news_df['TEXT'].apply(lambda x:x.lower())\n",
    "news_df['TEXT'] = news_df['TEXT'].apply(lambda x:re.sub('[^a-zA-z0-9\\s]','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(tokenize='spacy',tokenizer_language='en_core_web_sm')\n",
    "LABEL = LabelField(dtype=torch.float32)\n",
    "Fields =[(None, None),('LABEL',LABEL),('TEXT',TEXT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_ds = TabularDataset(\n",
    "    path='D:/py-py-py-pytorch/rnn/news_data.csv', format='csv',\n",
    "    skip_header=True, fields=Fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44898"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 40408\n",
      "Num Test: 4490\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = news_ds.split(\n",
    "    split_ratio=[0.9, 0.1],\n",
    "    random_state=random.seed(47))\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 32730\n",
      "Num Validation: 7678\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = train_data.split(\n",
    "    split_ratio=[0.81, 0.19],\n",
    "    random_state=random.seed(47))\n",
    "\n",
    "print(f'Num Train: {len(train_data)}')\n",
    "print(f'Num Validation: {len(valid_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LABEL': '0', 'TEXT': ['sickening', 'reason', 'cophating', 'racist', '49ers', 'qb', 'colin', 'kaepernick', 'just', 'announced', 'hell', 'now', 'stand', 'for', 'national', 'anthemit', 'looks', 'like', 'veteran', 'san', 'francisco', '49ers', 'quarterback', 'colin', 'kaepernick', 'might', 'be', 'done', 'with', 'his', 'national', 'anthem', 'protests', '  ', 'now', 'that', 'he', 's', 'looking', 'for', 'a', 'new', 'nfl', 'contractthe', 'controversial', 'player', 'will', 'stand', 'for', ' ', 'the', 'starspangled', 'banner', ' ', 'next', 'season', 'according', 'to', 'sources', 'who', 'spoke', 'to', 'espn', 'thursday', 'kaepernick', 'first', 'made', 'headlines', 'in', 'august', '2016', 'for', 'refusing', 'to', 'stand', 'during', 'his', 'team', 's', 'preseason', 'game', 'against', 'the', 'green', 'bay', 'packers', 'citing', 'the', ' ', 'oppression', ' ', 'of', 'black', 'people', 'in', 'the', 'united', 'stateskaepernick', 'explained', 'his', 'refusal', 'to', 'stand', 'for', 'the', 'starspangled', 'banner', 'expressing', 'solidarity', 'with', 'the', 'neomarxist', 'racial', 'narratives', 'of', 'black', 'lives', 'matter', 'i', 'am', 'not', 'going', 'to', 'stand', 'up', 'to', 'show', 'pride', 'in', 'a', 'flag', 'for', 'a', 'country', 'that', 'oppresses', 'black', 'people', 'and', 'people', 'of', 'color', 'to', 'me', 'this', 'is', 'bigger', 'than', 'football', 'and', 'it', 'would', 'be', 'selfish', 'on', 'my', 'part', 'to', 'look', 'the', 'other', 'way', 'there', 'are', 'bodies', 'in', 'the', 'street', 'and', 'people', 'getting', 'paid', 'leave', 'and', 'getting', 'away', 'with', 'murder', 'describing', 'himself', 'as', 'a', 'black', 'man', 'in', 'a', 'society', 'that', ' ', 'oppresses', 'black', 'people', ' ', 'kaepernick', 'signed', 'a', '6year', 'contract', 'with', 'the', '49ers', 'in', '2015', 'for', '114', 'million', 'he', 'has', 'also', 'been', 'paid', 'millions', 'of', 'additional', 'dollars', 'through', 'endorsement', 'dealsin', 'the', 'days', 'after', 'kaepernick', 's', 'first', 'protest', 'fans', 'began', 'burning', 'their', '49ersthemed', 'gear', 'one', 'lifelong', 'san', 'francisco', 'fan', 'set', 'his', 'kaepernick', 'jersey', 'up', 'in', 'flames', 'slamming', 'the', 'athlete', 'for', 'claiming', 'to', 'be', ' ', 'oppressed', ' ', 'when', 'he', 's', ' ', 'making', '126', 'million', 'and', 'in', 'september', 'nfl', 'commissioner', 'roger', 'goodell', 'said', 'that', 'while', 'the', 'league', 'supports', 'players', 'who', ' ', 'want', 'to', 'see', 'change', 'in', 'society', ' ', 'the', 'organization', 'believes', ' ', 'very', 'strongly', 'in', 'patriotism', 'i', 'personally', 'believe', 'very', 'strongly', 'in', 'that', ' ', 'he', 'said', 'according', 'to', 'usa', 'today', ' ', 'i', 'think', 'it', 's', 'important', 'to', 'have', 'respect', 'for', 'our', 'country', 'for', 'our', 'flag', 'for', 'the', 'people', 'who', 'make', 'our', 'country', 'better', 'for', 'law', 'enforcement', 'and', 'for', 'our', 'military', 'who', 'are', 'out', 'fighting', 'for', 'our', 'freedoms', 'and', 'our', 'ideals', 'while', 'kaepernick', 'received', 'kudos', 'from', 'those', 'on', 'the', 'left', 'many', 'fans', '  ', 'and', 'even', 'nfl', 'insiders', '  ', 'weren', 't', 'too', 'fond', 'of', 'the', 'protest', 'so', 'it', 's', 'no', 'surprise', 'he', 's', 'changing', 'his', 'ways', 'now', 'that', 'he', 's', 'looking', 'for', 'a', 'new', 'jobaccording', 'to', 'the', 'espn', 'report', 'kaepernick', ' ', 'no', 'longer', 'wants', 'his', 'method', 'of', 'protest', 'to', 'detract', 'from', 'the', 'positive', 'change', 'he', 'believes', 'has', 'been', 'created', '   ', 'a', 'change', 'that', 'comes', 'one', 'day', 'after', 'the', 'nfl', 'network', 's', 'ian', 'rapport', 'revealed', 'that', 'the', 'veteran', 'quarterback', 'has', 'decided', 'to', 'opt', 'out', 'of', 'his', '49ers', 'contract', 'and', 'is', 'now', 'a', 'free', 'agent', ' ', 'the', 'blaze']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabualry Size is 205614\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data,max_size=205614)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(\"Vocabualry Size is %d\" % len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 687383), ('to', 373633), ('of', 304821), ('a', 279350), ('and', 278517), (' ', 253551), ('in', 240090), ('that', 162418), ('on', 133236), ('for', 121885), ('s', 121271), ('is', 114708), ('he', 91309), ('said', 88706), ('trump', 86253), ('it', 84920), ('with', 81824), ('was', 79164), ('as', 70735), ('his', 66202)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl,valid_dl,test_dl = BucketIterator.splits((train_data,valid_data,test_data),\n",
    "                                           batch_size=100,\n",
    "                                           sort_within_batch=False,\n",
    "                                           sort_key=lambda x: len(x.TEXT),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_dl)\n",
    "iter_comp = next(it)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embedding GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "# loaded_glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "# loaded_glove_embeddings = loaded_glove_model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.GloveEmbeddings import create_custom_glove_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Glove_embeddings = create_custom_glove_embedding(TEXT.vocab,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Glove_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN-LSTM Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM(nn.Module):\n",
    "    '''\n",
    "    Building an RNN_LSTM model \n",
    "    '''\n",
    "    def __init__(self,input_dim,embedding_dim,hidden_dim,output_dim,num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        #Embedding \n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embeddings=torch.FloatTensor(Glove_embeddings),freeze= False)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        embed = self.embedding_layer(x)\n",
    "\n",
    "        out,(hidden,_) = self.lstm(embed)\n",
    "\n",
    "        flattened = hidden[-1]\n",
    "\n",
    "        out = torch.squeeze(self.fc(flattened))\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_LSTM(input_dim=len(TEXT.vocab),\n",
    "                 num_layers=3,\n",
    "                 embedding_dim=300,\n",
    "                 hidden_dim=256,\n",
    "                 output_dim=2)\n",
    "\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_LSTM(\n",
      "  (embedding_layer): Embedding(205614, 300)\n",
      "  (lstm): LSTM(300, 256, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Embedding matrix shape\n",
    "# it = iter(model.parameters())\n",
    "# next(it)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_properties(0).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "optimizer = Adam(model.parameters(),lr=0.0025)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training_loop_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\py-py-py-pytorch\\rnn\\rnn_impl_toy copy.ipynb Cell 47\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#X64sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#X64sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#forward\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#X64sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m preds,_ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#X64sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# print(preds.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#X64sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# print(labels.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#X64sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#X64sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(preds,labels)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    valid_acc = 0.0\n",
    "    train_pred, train_orig, valid_pred, valid_orig = [], [], [], []\n",
    "\n",
    "    for i, batch_data in enumerate(train_dl):\n",
    "        text = batch_data.TEXT.to(device)\n",
    "        labels = batch_data.LABEL.type(torch.LongTensor)\n",
    "        labels = labels.to(device)   \n",
    "        text = text.permute(1,0)\n",
    "        #forward\n",
    "        preds,_ = model.forward(text)\n",
    "        # print(preds.shape)\n",
    "        # print(labels.shape)\n",
    "        #loss\n",
    "        loss = loss_fn(preds,labels)\n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #update wts\n",
    "        optimizer.step()\n",
    "\n",
    "        output = F.softmax(preds)\n",
    "        _, pred = torch.max(output, dim=1)\n",
    "        correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "        accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "        train_acc += accuracy.item() * text.size(0)\n",
    "        #print(\"Epoch {} || Accuracy {}\".format(epoch+1, train_acc))\n",
    "        train_pred.extend(pred.cpu().numpy())\n",
    "        train_orig.extend(labels.cpu().numpy())\n",
    "        train_acc = train_acc / len(train_dl.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training_loop_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_acc = []\n",
    "# train_loss = []\n",
    "\n",
    "# valid_acc = []\n",
    "# valid_loss = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(\"Epoch {}  |\".format(epoch+1))\n",
    "#     epoch_loss = []\n",
    "#     epoch_acc = []\n",
    "#     for text,label in train_dl:\n",
    "        \n",
    "#         batch_loss = []\n",
    "#         batch_acc = []\n",
    "#         # text.permute(1,0)\n",
    "#         optimizer.zero_grad()\n",
    "#         pred = model(text)\n",
    "#         print(pred.shape,label.shape)\n",
    "#         loss = loss_fn(pred,label)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         batch_loss.append(loss.item())\n",
    "\n",
    "#         correct_preds = sum((torch.sigmoid(pred)>0.5) == label)\n",
    "#         acc = correct_preds/len(label)\n",
    "#         batch_acc.append(acc.item())\n",
    "    \n",
    "#     mean_epoch_loss = np.mean(batch_loss)\n",
    "#     mean_epoch_acc = np.mean(batch_acc)\n",
    "\n",
    "#     print(\"Train Accuracy: {} | Train Loss {} \".format(mean_epoch_acc,mean_epoch_loss))\n",
    "\n",
    "#     epoch_loss.append(mean_epoch_loss)\n",
    "#     epoch_acc.append(mean_epoch_acc)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, criterion, dataloader):\n",
    "  model.train()\n",
    "  losses = []\n",
    "  accs = []\n",
    "  for i, (x, y) in enumerate(dataloader):\n",
    "      opt.zero_grad()\n",
    "      # Forward pass\n",
    "      x = x.to(device)\n",
    "      x = x.permute(1,0)\n",
    "      y = y.to(device)\n",
    "      pred = model(x).to(device)\n",
    "      # Loss Computation\n",
    "      loss = criterion(pred, y)\n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "      # Weights update\n",
    "      opt.step()\n",
    "      losses.append(loss.item())\n",
    "      # Compute accuracy\n",
    "      num_corrects = sum((torch.sigmoid(pred)>0.5) == y)\n",
    "      acc = 100.0 * num_corrects/len(y)\n",
    "      accs.append(acc.item())\n",
    "      if (i%20 == 0):\n",
    "          # print(\"Batch \" + str(i) + \" : training loss = \" + str(loss.item()) + \"; training acc = \" + str(acc.item()))\n",
    "          print(\"Batch: {}    |    Train Loss: {:.3f}    |    Train Acc: {:.3f} \".format(i,loss.item(),acc.item()))\n",
    "  return losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, criterion, evalloader):\n",
    "  model.eval()\n",
    "  total_epoch_loss = 0\n",
    "  total_epoch_acc = 0\n",
    "  preds = []\n",
    "  with torch.no_grad():\n",
    "      for i, (x, y) in enumerate(evalloader):\n",
    "          pred = model(x)\n",
    "          loss = criterion(pred, y)\n",
    "          num_corrects = sum((torch.sigmoid(pred)>0.5) == y)\n",
    "          acc = 100.0 * num_corrects/len(y)\n",
    "          total_epoch_loss += loss.item()\n",
    "          total_epoch_acc += acc.item()\n",
    "          preds.append(pred)\n",
    "\n",
    "  return total_epoch_loss/(i+1), total_epoch_acc/(i+1), preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opt, criterion, num_epochs = 5):\n",
    "  train_losses = []\n",
    "  valid_losses = []\n",
    "  train_accs = []\n",
    "  valid_accs = []\n",
    "  print(\"Beginning training...\")\n",
    "  for e in range(num_epochs):\n",
    "      print(\"Epoch \" + str(e+1) + \":\")\n",
    "      losses, accs = train_epoch(model, opt, criterion, train_dl)\n",
    "      train_losses.append(losses)\n",
    "      train_accs.append(accs)\n",
    "  #     valid_loss, valid_acc, val_preds = eval_model(model, criterion, valid_dl)\n",
    "  #     valid_losses.append(valid_loss)\n",
    "  #     valid_accs.append(valid_acc)\n",
    "  #     # print(\"Epoch \" + str(e+1) + \" : Validation loss = \" + str(valid_loss) + \"; Validation acc = \" + str(valid_acc))\n",
    "  #     print(\"Epoch: {}    |    Validation Loss: {:.3f}    |    Validation Acc: {:.3f} \".format(e+1,valid_loss,valid_acc))\n",
    "  # test_loss, test_acc, test_preds = eval_model(model, criterion, test_dl)\n",
    "  # print(\"Test loss = \" + str(test_loss) + \"; Test acc = \" + str(test_acc))\n",
    "  # return train_losses, valid_losses, test_loss, train_accs, valid_accs, test_acc, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300 \n",
    "VOCAB_SIZE = len(vocab_dict)\n",
    "HIDDEN_DIM = 256\n",
    "learning_rate = 0.0025\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_LSTM(input_dim=len(TEXT.vocab),\n",
    "                 num_layers=3,\n",
    "                 embedding_dim=300,\n",
    "                 hidden_dim=256,\n",
    "                 output_dim=1)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "Epoch 1:\n",
      "Batch: 0    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 20    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 40    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 60    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 80    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 100    |    Train Loss: 0.695    |    Train Acc: 40.625 \n",
      "Batch: 120    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 140    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 160    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 180    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 200    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 220    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 240    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 260    |    Train Loss: 0.698    |    Train Acc: 31.250 \n",
      "Batch: 280    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 300    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 320    |    Train Loss: 0.693    |    Train Acc: 53.125 \n",
      "Batch: 340    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 360    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 380    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 400    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 420    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 440    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 460    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 480    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 500    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 520    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 540    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 560    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 580    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 600    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 620    |    Train Loss: 0.695    |    Train Acc: 40.625 \n",
      "Batch: 640    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 660    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 680    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 700    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 720    |    Train Loss: 0.699    |    Train Acc: 28.125 \n",
      "Batch: 740    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 760    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 780    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 800    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 820    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 840    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 860    |    Train Loss: 0.689    |    Train Acc: 65.625 \n",
      "Batch: 880    |    Train Loss: 0.698    |    Train Acc: 31.250 \n",
      "Batch: 900    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 920    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 940    |    Train Loss: 0.695    |    Train Acc: 40.625 \n",
      "Epoch 2:\n",
      "Batch: 0    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 20    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 40    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 60    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 80    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 100    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 120    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 140    |    Train Loss: 0.695    |    Train Acc: 40.625 \n",
      "Batch: 160    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 180    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 200    |    Train Loss: 0.695    |    Train Acc: 40.625 \n",
      "Batch: 220    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 240    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 260    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 280    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 300    |    Train Loss: 0.698    |    Train Acc: 31.250 \n",
      "Batch: 320    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 340    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 360    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 380    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 400    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 420    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 440    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 460    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 480    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 500    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 520    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 540    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 560    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 580    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 600    |    Train Loss: 0.695    |    Train Acc: 40.625 \n",
      "Batch: 620    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 640    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 660    |    Train Loss: 0.693    |    Train Acc: 53.125 \n",
      "Batch: 680    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 700    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 720    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 740    |    Train Loss: 0.689    |    Train Acc: 65.625 \n",
      "Batch: 760    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 780    |    Train Loss: 0.693    |    Train Acc: 53.125 \n",
      "Batch: 800    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 820    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 840    |    Train Loss: 0.689    |    Train Acc: 65.625 \n",
      "Batch: 860    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 880    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 900    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 920    |    Train Loss: 0.699    |    Train Acc: 28.125 \n",
      "Batch: 940    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Epoch 3:\n",
      "Batch: 0    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 20    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 40    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 60    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 80    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 100    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 120    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 140    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 160    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 180    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 200    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 220    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 240    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 260    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 280    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 300    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 320    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 340    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 360    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 380    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 400    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 420    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 440    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 460    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 480    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 500    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 520    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 540    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 560    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 580    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 600    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 620    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 640    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 660    |    Train Loss: 0.698    |    Train Acc: 31.250 \n",
      "Batch: 680    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 700    |    Train Loss: 0.693    |    Train Acc: 53.125 \n",
      "Batch: 720    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 740    |    Train Loss: 0.699    |    Train Acc: 28.125 \n",
      "Batch: 760    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 780    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 800    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 820    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 840    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 860    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 880    |    Train Loss: 0.688    |    Train Acc: 71.875 \n",
      "Batch: 900    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 920    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 940    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Epoch 4:\n",
      "Batch: 0    |    Train Loss: 0.695    |    Train Acc: 40.625 \n",
      "Batch: 20    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 40    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 60    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 80    |    Train Loss: 0.699    |    Train Acc: 28.125 \n",
      "Batch: 100    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 120    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 140    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 160    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 180    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 200    |    Train Loss: 0.693    |    Train Acc: 53.125 \n",
      "Batch: 220    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 240    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 260    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 280    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 300    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 320    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 340    |    Train Loss: 0.695    |    Train Acc: 40.625 \n",
      "Batch: 360    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 380    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 400    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 420    |    Train Loss: 0.700    |    Train Acc: 21.875 \n",
      "Batch: 440    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 460    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 480    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 500    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 520    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 540    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 560    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 580    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 600    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 620    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 640    |    Train Loss: 0.693    |    Train Acc: 53.125 \n",
      "Batch: 660    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 680    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 700    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 720    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 740    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 760    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 780    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 800    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 820    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 840    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 860    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 880    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 900    |    Train Loss: 0.688    |    Train Acc: 71.875 \n",
      "Batch: 920    |    Train Loss: 0.699    |    Train Acc: 25.000 \n",
      "Batch: 940    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Epoch 5:\n",
      "Batch: 0    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 20    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 40    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 60    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 80    |    Train Loss: 0.698    |    Train Acc: 31.250 \n",
      "Batch: 100    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 120    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 140    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 160    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 180    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 200    |    Train Loss: 0.689    |    Train Acc: 65.625 \n",
      "Batch: 220    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 240    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 260    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 280    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 300    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 320    |    Train Loss: 0.698    |    Train Acc: 31.250 \n",
      "Batch: 340    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 360    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 380    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 400    |    Train Loss: 0.688    |    Train Acc: 71.875 \n",
      "Batch: 420    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 440    |    Train Loss: 0.689    |    Train Acc: 65.625 \n",
      "Batch: 460    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 480    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 500    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 520    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 540    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 560    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 580    |    Train Loss: 0.700    |    Train Acc: 21.875 \n",
      "Batch: 600    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 620    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 640    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 660    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 680    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 700    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 720    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 740    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 760    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 780    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 800    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 820    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 840    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 860    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 880    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 900    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 920    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 940    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Epoch 6:\n",
      "Batch: 0    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 20    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 40    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 60    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 80    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 100    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 120    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 140    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 160    |    Train Loss: 0.698    |    Train Acc: 31.250 \n",
      "Batch: 180    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 200    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 220    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 240    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 260    |    Train Loss: 0.693    |    Train Acc: 53.125 \n",
      "Batch: 280    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 300    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 320    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 340    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 360    |    Train Loss: 0.689    |    Train Acc: 65.625 \n",
      "Batch: 380    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 400    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 420    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 440    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 460    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 480    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 500    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 520    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 540    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 560    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 580    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 600    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 620    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 640    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 660    |    Train Loss: 0.695    |    Train Acc: 40.625 \n",
      "Batch: 680    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 700    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 720    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 740    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 760    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 780    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 800    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 820    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 840    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 860    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 880    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 900    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 920    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 940    |    Train Loss: 0.695    |    Train Acc: 40.625 \n",
      "Epoch 7:\n",
      "Batch: 0    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 20    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 40    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 60    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 80    |    Train Loss: 0.689    |    Train Acc: 65.625 \n",
      "Batch: 100    |    Train Loss: 0.699    |    Train Acc: 25.000 \n",
      "Batch: 120    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 140    |    Train Loss: 0.689    |    Train Acc: 68.750 \n",
      "Batch: 160    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 180    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 200    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 220    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 240    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 260    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 280    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 300    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 320    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 340    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 360    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 380    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 400    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 420    |    Train Loss: 0.693    |    Train Acc: 53.125 \n",
      "Batch: 440    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 460    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 480    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 500    |    Train Loss: 0.688    |    Train Acc: 71.875 \n",
      "Batch: 520    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 540    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 560    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 580    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 600    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 620    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 640    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 660    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 680    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 700    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 720    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 740    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 760    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 780    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 800    |    Train Loss: 0.689    |    Train Acc: 65.625 \n",
      "Batch: 820    |    Train Loss: 0.698    |    Train Acc: 31.250 \n",
      "Batch: 840    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 860    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 880    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 900    |    Train Loss: 0.695    |    Train Acc: 40.625 \n",
      "Batch: 920    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 940    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Epoch 8:\n",
      "Batch: 0    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 20    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 40    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 60    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 80    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 100    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 120    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 140    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 160    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 180    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 200    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 220    |    Train Loss: 0.689    |    Train Acc: 65.625 \n",
      "Batch: 240    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 260    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 280    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 300    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 320    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 340    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 360    |    Train Loss: 0.695    |    Train Acc: 40.625 \n",
      "Batch: 380    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 400    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 420    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 440    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 460    |    Train Loss: 0.691    |    Train Acc: 59.375 \n",
      "Batch: 480    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 500    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 520    |    Train Loss: 0.697    |    Train Acc: 34.375 \n",
      "Batch: 540    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 560    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 580    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 600    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 620    |    Train Loss: 0.690    |    Train Acc: 62.500 \n",
      "Batch: 640    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 660    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 680    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 700    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 720    |    Train Loss: 0.695    |    Train Acc: 43.750 \n",
      "Batch: 740    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 760    |    Train Loss: 0.692    |    Train Acc: 53.125 \n",
      "Batch: 780    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 800    |    Train Loss: 0.694    |    Train Acc: 46.875 \n",
      "Batch: 820    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 840    |    Train Loss: 0.689    |    Train Acc: 68.750 \n",
      "Batch: 860    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 880    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 900    |    Train Loss: 0.692    |    Train Acc: 56.250 \n",
      "Batch: 920    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 940    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Epoch 9:\n",
      "Batch: 0    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 20    |    Train Loss: 0.696    |    Train Acc: 40.625 \n",
      "Batch: 40    |    Train Loss: 0.696    |    Train Acc: 37.500 \n",
      "Batch: 60    |    Train Loss: 0.693    |    Train Acc: 50.000 \n",
      "Batch: 80    |    Train Loss: 0.693    |    Train Acc: 50.000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\py-py-py-pytorch\\rnn\\rnn_impl_toy copy.ipynb Cell 59\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, optimizer, loss_fn, num_epochs)\n",
      "\u001b[1;32md:\\py-py-py-pytorch\\rnn\\rnn_impl_toy copy.ipynb Cell 59\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#Y110sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#Y110sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#Y110sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     losses, accs \u001b[39m=\u001b[39m train_epoch(model, opt, criterion, train_dl)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#Y110sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(losses)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#Y110sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_accs\u001b[39m.\u001b[39mappend(accs)\n",
      "\u001b[1;32md:\\py-py-py-pytorch\\rnn\\rnn_impl_toy copy.ipynb Cell 59\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#Y110sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Weights update\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#Y110sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#Y110sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#Y110sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Compute accuracy\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/py-py-py-pytorch/rnn/rnn_impl_toy%20copy.ipynb#Y110sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m num_corrects \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m((torch\u001b[39m.\u001b[39msigmoid(pred)\u001b[39m>\u001b[39m\u001b[39m0.5\u001b[39m) \u001b[39m==\u001b[39m y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer, loss_fn, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_exec_2.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
